{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df380d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments,Trainer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from collections import Counter\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel, PeftConfig\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import sys\n",
    "from IPython.display import Image, display\n",
    "\n",
    "sys.path.insert(1, '../')\n",
    "from utils.helper import Helper\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4aa182e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "Summary  Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'knkarthick/dialogsum'\n",
    "dataset = load_dataset(dataset_name)\n",
    "path_to_data = '../data/'\n",
    "split = 'test'\n",
    "print(dataset[split][0]['dialogue'])\n",
    "print(\"Summary \" , dataset['train'][0]['summary'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7945aeb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbebb5f",
   "metadata": {},
   "source": [
    "# Zero Shot Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f577116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dialoge: \n",
      "  \n",
      "Dialogue: \n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm. \n",
      "What was going on? \n",
      "\n",
      "\n",
      " Summary: \n",
      " Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
      "\n",
      " Output: \n",
      " The memo is to be distributed to all employees by this afternoon.\n",
      "-----------------------------------------------\n",
      "\n",
      " Dialoge: \n",
      "  \n",
      "Dialogue: \n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm. \n",
      "What was going on? \n",
      "\n",
      "\n",
      " Summary: \n",
      " In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.\n",
      "\n",
      " Output: \n",
      " The memo is to be distributed to all employees by this afternoon.\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "example_indicies = [0,1]\n",
    "for i,index in enumerate(example_indicies):\n",
    "    dialoge = dataset[split][index]['dialogue']\n",
    "    summary = dataset[split][index]['summary']\n",
    "    \n",
    "    prompt = f\"\"\" \n",
    "Dialogue: \n",
    "{dialoge} \n",
    "What was going on? \n",
    "\"\"\"\n",
    "    inputs  = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(model.generate(inputs['input_ids'],max_new_tokens=50)[0],skip_special_tokens=True)\n",
    "    print(\"\\n Dialoge: \\n\" , prompt)\n",
    "    print(\"\\n Summary: \\n\" , summary)\n",
    "    print(\"\\n Output: \\n\"  , output)\n",
    "    print(\"-----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d29e5b3",
   "metadata": {},
   "source": [
    "# One Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dfe83df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dialoge: \n",
      "  \n",
      "Dialogue: \n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm. \n",
      "What was going on? \n",
      "\n",
      "\n",
      " Output: \n",
      " The memo is to be distributed to all employees by this afternoon.\n",
      "-----------------------------------------------\n",
      "\n",
      " Dialoge: \n",
      "  \n",
      "Dialogue: \n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm. \n",
      "What was going on? \n",
      "\n",
      "\n",
      " Output: \n",
      " The memo is to be distributed to all employees by this afternoon.\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "example_indicies = [0,1]\n",
    "for i,index in enumerate(example_indicies):\n",
    "    dialoge = dataset[split][index]['dialogue']\n",
    "    summary = dataset[split][index]['summary']\n",
    "    \n",
    "    prompt = f\"\"\" \n",
    "Dialogue: \n",
    "{dialoge} \n",
    "What was going on? \n",
    "{summary}\n",
    "\"\"\"\n",
    "    prompt = f\"\"\" \n",
    "Dialogue: \n",
    "{dialoge} \n",
    "What was going on? \n",
    "\"\"\"\n",
    "    inputs  = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(model.generate(inputs['input_ids'],max_new_tokens=50)[0],skip_special_tokens=True)\n",
    "    print(\"\\n Dialoge: \\n\" , prompt)\n",
    "    print(\"\\n Output: \\n\"  , output)\n",
    "    print(\"-----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0db012",
   "metadata": {},
   "source": [
    "# Few shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f9d858f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (785 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dialoge: \n",
      "  \n",
      "Dialogue: \n",
      "#Person1#: Mr. Wilson. We are very regretful about the mistakes in goods. I am very sorry and we will be responsible for the mistake.\n",
      "#Person2#: We have no choice but to hold you responsible for the loss we sustained.\n",
      "#Person1#: The first problem is supposed to be solved after the investigation. About the second problem, I admit it's our fault, so we will exchange all merchandise that falls short of our sample.\n",
      "#Person2#: Well. I hope there won't be such things no more.\n",
      "#Person1#: I can assure you that such a thing today will never happen again in future delivery. We have made the plan to improve the package of our exported goods. \n",
      "What was going on? \n",
      "#Person1# apologizes for the loss caused by them to Mr. Wilson and assures that it will never happen again.\n",
      " \n",
      "Dialogue: \n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday \n",
      "What was going on? \n",
      "#Person1# and Brian are at the birthday party of Brian. Brian thinks #Person1# looks great and is popular.\n",
      " \n",
      "Dialogue: \n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm. \n",
      "What was going on? \n",
      "\n",
      "\n",
      " Output: \n",
      " #Person1 needs to take a dictation from Ms. Dawson.\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "example_indicies = [323,9]\n",
    "prompt = ''\n",
    "for i,index in enumerate(example_indicies):\n",
    "    dialoge = dataset[split][index]['dialogue']\n",
    "    summary = dataset[split][index]['summary']\n",
    "    \n",
    "    prompt += f\"\"\" \n",
    "Dialogue: \n",
    "{dialoge} \n",
    "What was going on? \n",
    "{summary}\n",
    "\"\"\"\n",
    "    \n",
    "index = 1\n",
    "dialoge = dataset[split][index]['dialogue']\n",
    "summary = dataset[split][index]['summary']\n",
    "\n",
    "prompt += f\"\"\" \n",
    "Dialogue: \n",
    "{dialoge} \n",
    "What was going on? \n",
    "\"\"\"\n",
    "\n",
    "inputs  = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(model.generate(inputs['input_ids'],max_new_tokens=50)[0],skip_special_tokens=True,do_sample=True, tempreture=0.5)\n",
    "print(\"\\n Dialoge: \\n\" , prompt)\n",
    "print(\"\\n Output: \\n\"  , output)\n",
    "print(\"-----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d766a553",
   "metadata": {},
   "source": [
    "# Fully Instruction Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05d06a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c352bec2041842afb182a9733107a60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f271035bd747c78601ba4947b7ae2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a10cd701474d2dad9d3259b4cb1ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e03e798aa1d47d59122c0d0bda3ba85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0634d909d242dca4333179613fdb83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d352418c7e4d7ebffefb4ec5f95281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "helper = Helper(tokenizer)\n",
    "tokenized_dataset = dataset.map(helper.tokenizer_function, batched=True)\n",
    "tokenized_datasets = tokenized_dataset.remove_columns(['id','topic','dialogue','summary'])\n",
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index : index % 2 == 0, with_indices = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015e6ee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/Users/raminanushiravani/miniconda3/envs/ramin/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 750\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1\n",
      "  Number of trainable parameters = 247577856\n"
     ]
    }
   ],
   "source": [
    "model_o = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\n",
    "\n",
    "training_args = TrainingArguments(output_dir = path_to_data+'summary_data/',\n",
    "                                 learning_rate=1e-5, \n",
    "                                 num_train_epochs=1, \n",
    "                                 weight_decay=0.01,\n",
    "                                 logging_steps=1,\n",
    "                                 max_steps=1,\n",
    "                                 auto_find_batch_size=True)\n",
    "\n",
    "trainer = Trainer(model = model_o, \n",
    "                 args = training_args, \n",
    "                 train_dataset= tokenized_datasets['test'])\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model('../data/Flan_T5_Full_Fine_Tuned' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5ba539",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3366ba9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_f = AutoModelForSeq2SeqLM.from_pretrained(path_to_data + 'Flan_T5_Full_Fine_Tuned')\n",
    "model_o = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base',use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf42c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"original\" , sum([p.numel()/1e6 for p in model_o.parameters() if p.requires_grad]) , 'M trainable parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d225c53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orgs = []\n",
    "fids = []\n",
    "gros = []\n",
    "for i in range(5,15):\n",
    "    prompt,gr_ = helper.create_prompt([1,2,3], i)\n",
    "    org_ , fid_ =  helper.gen_output(prompt, model_o, model_f)\n",
    "    orgs.append(org_)\n",
    "    fids.append(fid_)\n",
    "    gros.append(gr_)\n",
    "\n",
    "zipped_summaries = list(zip(gros,orgs,fids))\n",
    "df = pd.DataFrame(zipped_summaries,columns=['G','O','F'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2635847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "             predictions=orgs,\n",
    "             references=gros,\n",
    "             use_aggregator=True,\n",
    "             use_stemmer=True)\n",
    "\n",
    "fined_model_results = rouge.compute(\n",
    "             predictions=fids,\n",
    "             references=gros,\n",
    "             use_aggregator=True,\n",
    "             use_stemmer=True)\n",
    "\n",
    "print(\"Original\" , original_model_results)\n",
    "print(\"Fined\" , fined_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f7b57",
   "metadata": {},
   "source": [
    "# Parameter Efficient Fine-Tuning LoRA\n",
    "\n",
    "Low rank adaptation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926c8df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(tokenizer_function, batched=True)\n",
    "tokenized_datasets = tokenized_dataset.remove_columns(['id','topic','dialogue','summary'])\n",
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index : index % 2 == 0, with_indices = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37036be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(r = 32, \n",
    "                        lora_alpha = 32, \n",
    "                        target_modules=['q','v'],\n",
    "                        lora_dropout = 0.05,\n",
    "                        bias = \"none\",\n",
    "                        task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "\n",
    "peft_model = get_peft_model(model_o, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8893b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"peft\" ,sum([p.numel()/1e6 for p in peft_model.parameters()]) , 'M parameters')\n",
    "\n",
    "print(\"peft\" ,sum([p.numel()/1e6 for p in peft_model.parameters() if p.requires_grad]) , 'M trainable parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9096177",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_training_args = TrainingArguments(output_dir = path_to_data+'summary_data/',\n",
    "                                 learning_rate=1e-3, \n",
    "                                 num_train_epochs=10, \n",
    "                                 auto_find_batch_size=True,\n",
    "                                 weight_decay=0.01,\n",
    "                                 logging_steps=1,\n",
    "                                 max_steps=20)\n",
    "\n",
    "trainer = Trainer(model = peft_model, \n",
    "                 args = peft_training_args, \n",
    "                 train_dataset= tokenized_datasets['train'],\n",
    "                 eval_dataset = tokenized_datasets['test'])\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model('../data/Flan_T5_Lora_Fine_Tuned' )\n",
    "\n",
    "trainer.model.save_pretrained('../data/Flan_T5_Lora_Torch')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d2413",
   "metadata": {},
   "source": [
    "### inference Merge Peft with base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b0545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_o = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base',use_fast=True)\n",
    "\n",
    "# Merging \n",
    "peft_model_path = '../data/Flan_T5_Lora_Torch'\n",
    "peft_model = PeftModel.from_pretrained(model_o,peft_model_path, \n",
    "                                       torch_dype = torch.bfloat16,\n",
    "                                       is_trainable = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a2a11f",
   "metadata": {},
   "source": [
    "### Evaluation LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf74e74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_o = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\n",
    "\n",
    "\n",
    "orgs = []\n",
    "pfids = []\n",
    "gros = []\n",
    "for i in range(5,15):\n",
    "    prompt,gr_ = helper.create_prompt([1,2,3], i)\n",
    "    org_ , fid_ =  helper.gen_output(prompt, model_o, peft_model)\n",
    "    orgs.append(org_)\n",
    "    pfids.append(fid_)\n",
    "    gros.append(gr_)\n",
    "\n",
    "zipped_summaries = list(zip(gros,orgs,pfids))\n",
    "df = pd.DataFrame(zipped_summaries,columns=['G','O','PEF'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "             predictions=orgs,\n",
    "             references=gros,\n",
    "             use_aggregator=True,\n",
    "             use_stemmer=True)\n",
    "\n",
    "fined_model_results = rouge.compute(\n",
    "             predictions=pfids,\n",
    "             references=gros,\n",
    "             use_aggregator=True,\n",
    "             use_stemmer=True)\n",
    "\n",
    "print(\"Original\" , original_model_results)\n",
    "print(\"PEFT\" , fined_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66b4cee",
   "metadata": {},
   "source": [
    "# Fine Tune with RLHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6109b320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\n",
    "from trl import create_reference_model\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29125da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'knkarthick/dialogsum'\n",
    "model_name = 'google/flan-t5-base'\n",
    "dataset = helper.build_dataset(model_name,dataset_name,minLen=200,maxLen=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceff3551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model_o = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\n",
    "\n",
    "lora_config = LoraConfig(r = 32, \n",
    "                        lora_alpha = 32, \n",
    "                        target_modules=['q','v'],\n",
    "                        lora_dropout = 0.05,\n",
    "                        bias = \"none\",\n",
    "                        task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "\n",
    "peft_model_path = '../data/Flan_T5_Lora_Torch'\n",
    "peft_model = PeftModel.from_pretrained(model_o,\n",
    "                                       peft_model_path, \n",
    "                                       lora_config = lora_config,\n",
    "                                       torch_dype = torch.bfloat16,\n",
    "                                       is_trainable = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d3b8d4",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bf8b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='../snapshots/ppo.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca52845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppo model\n",
    "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model,\n",
    "                                                               torch_dtyoe = torch.bfloat16, \n",
    "                                                               is_trainable = True\n",
    "                                                              ,device_map = 'auto')\n",
    "# reference model \n",
    "ref_model = create_reference_model(ppo_model)\n",
    "\n",
    "# Later we use KL divergence to compare the output from ppo model to the reference model \n",
    "\n",
    "print(ppo_model.v_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404aff61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toxic_model_name = 'facebook/roberta-hate-speech-dynabench-r4-target'\n",
    "toxic_tokenizer = AutoTokenizer.from_pretrained(toxic_model_name,device_map = 'auto')\n",
    "toxic_model = AutoModelForSequenceClassification.from_pretrained(toxic_model_name, device_map = 'auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252636a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "sentiment_pipe = pipeline('sentiment-analysis',model=toxic_model_name, device = device)\n",
    "\n",
    "reward_logits_kwargs = {\"top_k\" : None, \n",
    "                        \"function_to_apply\": \"none\",\n",
    "                        \"batch_size\" : 16}\n",
    "\n",
    "reward_probs_kwargs = {\"top_k\" : None, \n",
    "                        \"function_to_apply\": \"softmax\",\n",
    "                        \"batch_size\" : 16}\n",
    "\n",
    "toxic_eval  = evaluate.load(\"toxicity\", toxic_model_name, model_type='measurement',toxic_label= \"hate\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccfca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map = 'auto')\n",
    "mean_b_toxic, std_b_toxic = evaluate_toxic(ref_model, toxic_eval=toxic_eval, \n",
    "                                           tokenizer=tokenizer, dataset = dataset['test'],\n",
    "                                           num_samples = 10)\n",
    "print(f'toxic [mean,std] before detox [{mean_b_toxic},{std_b_toxic}]')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c3e218",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1.4e-5\n",
    "max_ppo_epochs = 1\n",
    "mini_batch_size = 4,\n",
    "batch_size = 16\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name = model_name,\n",
    "    learning_rate = lr,\n",
    "    ppo_epochs =max_ppo_epochs, \n",
    "    mini_batch_size=mini_batch_size,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "ppo_trainer = PPOTrainer(config=config, \n",
    "                        model=ppo_model, \n",
    "                        ref_model=ref_model, \n",
    "                        tokenizer=tokenizer, \n",
    "                        dataset=dataset['train'],\n",
    "                        data_collator=collator\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486dfb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_min_len = 100\n",
    "output_max_len = 400\n",
    "output_len_sampler = LengthSampler(output_min_len, output_max_len)\n",
    "\n",
    "generation_kwargs = {\n",
    "                    \"min_length\":5,\n",
    "                    \"top_k\": 0.0,\n",
    "                    \"top_p\":1.0,\n",
    "                    \"do_sample\":True}\n",
    "\n",
    "max_ppo_steps = 10\n",
    "\n",
    "for step,batch in enumerate(ppo_trainer.dataloader):\n",
    "    if step >= max_ppo_steps: \n",
    "        break\n",
    "    prompt_tensors = batch['input_ids']\n",
    "    summary_tensors = []\n",
    "    for prompt_tensor in prompt_tensors:\n",
    "        max_new_tokens = output_len_sampler()\n",
    "        generation_kwargs['max_new_tokens'] = max_new_tokens\n",
    "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
    "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
    "    \n",
    "    batch['response'] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
    "    query_response_pair = [q+r for q,r in zip(batch['query'],batch['response'])]\n",
    "    rewards = sentiment_pipe(query_response_pairs, **reward_kwargs)\n",
    "    \n",
    "    reward_tensors = [torch.tensor(reward[not_hate_index]['score']) for reward in rewards]\n",
    "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
    "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26cf78b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
